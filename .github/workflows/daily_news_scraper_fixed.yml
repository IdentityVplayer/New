name: Daily News Scraper

on:
  schedule:
    - cron: '0 0 * * *'  # æ¯æ—¥0:00æ‰§è¡Œ
  workflow_dispatch:  # å…è®¸æ‰‹åŠ¨è§¦å‘

jobs:
  scrape-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 jinja2 lxml
        
    - name: Create news scraping script
      run: |
        cat > scrape_news.py << 'EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import os
import re
import json
import time
import random
import requests
from datetime import datetime
from bs4 import BeautifulSoup
from jinja2 import Template

def setup_directories():
    """åˆ›å»ºå¿…è¦çš„ç›®å½•ç»“æ„"""
    today = datetime.now()
    year_month = today.strftime("%Y/%m")
    
    base_dir = "news_data"
    html_dir = f"html/{year_month}"
    
    os.makedirs(base_dir, exist_ok=True)
    os.makedirs(html_dir, exist_ok=True)
    
    return base_dir, html_dir

def get_user_agent():
    """è·å–éšæœºUser-Agent"""
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    ]
    return random.choice(user_agents)

def scrape_tophub_news():
    """æŠ“å–TopHubæ–°é—»æ•°æ®"""
    url = "https://tophub.today/c/news"
    
    headers = {
        'User-Agent': get_user_agent(),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'Accept-Encoding': 'gzip, deflate, br',
        'DNT': '1',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
    }
    
    try:
        print(f"æ­£åœ¨è®¿é—®: {url}")
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        
        response.encoding = response.apparent_encoding or 'utf-8'
        
        soup = BeautifulSoup(response.text, 'html.parser')
        news_items = []
        
        # å°è¯•å¤šç§é€‰æ‹©å™¨æ¨¡å¼æŠ“å–æ–°é—»
        selectors = [
            '.news-item',
            '.item',
            '.news-list .item',
            '.content-item',
            'li a[href*="news"]',
            '.link-item'
        ]
        
        items = []
        for selector in selectors:
            items = soup.select(selector)
            if items:
                print(f"æ‰¾åˆ° {len(items)} ä¸ªæ–°é—»é¡¹ç›®ï¼Œä½¿ç”¨é€‰æ‹©å™¨: {selector}")
                break
        
        if not items:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šæŸ¥æ‰¾æ‰€æœ‰é“¾æ¥
            items = soup.find_all('a', href=True)
            print(f"ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼Œæ‰¾åˆ° {len(items)} ä¸ªé“¾æ¥")
        
        for item in items[:50]:  # é™åˆ¶50æ¡æ–°é—»
            try:
                title = ""
                link = ""
                source = ""
                
                if item.name == 'a':
                    title = item.get_text(strip=True)
                    link = item.get('href', '')
                else:
                    link_elem = item.find('a')
                    if link_elem:
                        title = link_elem.get_text(strip=True)
                        link = link_elem.get('href', '')
                    else:
                        title = item.get_text(strip=True)
                
                # è¿‡æ»¤æ— æ•ˆæ•°æ®
                if len(title) > 5 and link:
                    # è§„èŒƒåŒ–é“¾æ¥
                    if link.startswith('http'):
                        pass
                    elif link.startswith('/'):
                        link = "https://tophub.today" + link
                    else:
                        link = "https://tophub.today/" + link
                    
                    # æå–æ¥æºä¿¡æ¯
                    source_match = re.search(r'([^/]+://[^/]+)', link)
                    source = source_match.group(1) if source_match else "TopHub"
                    
                    news_items.append({
                        'title': title,
                        'link': link,
                        'source': source,
                        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
                    
            except Exception as e:
                print(f"è§£ææ–°é—»é¡¹ç›®æ—¶å‡ºé”™: {e}")
                continue
        
        print(f"æˆåŠŸè§£æ {len(news_items)} æ¡æ–°é—»")
        return news_items
        
    except requests.RequestException as e:
        print(f"è¯·æ±‚é”™è¯¯: {e}")
        return []
    except Exception as e:
        print(f"æŠ“å–å¤±è´¥: {e}")
        return []

def generate_html(news_items, output_dir):
    """ç”ŸæˆHTMLæ–‡ä»¶"""
    if not news_items:
        print("æ²¡æœ‰æ–°é—»æ•°æ®ï¼Œè·³è¿‡HTMLç”Ÿæˆ")
        return None
        
    today = datetime.now()
    filename = f"news-{today.strftime('%Y-%m-%d')}.html"
    filepath = os.path.join(output_dir, filename)
    
    # HTMLæ¨¡æ¿
    html_template = '''<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¯æ—¥æ–°é—» - {{ date }}</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'PingFang SC', 'Hiragino Sans GB', 'Microsoft YaHei', 'Helvetica Neue', Helvetica, Arial, sans-serif;
            line-height: 1.6; color: #333; background-color: #f5f5f5;
        }
        .container {
            max-width: 1200px; margin: 0 auto; padding: 20px;
            background: white; box-shadow: 0 2px 10px rgba(0,0,0,0.1);
            margin-top: 20px; border-radius: 8px;
        }
        .header {
            text-align: center; margin-bottom: 30px; padding-bottom: 20px;
            border-bottom: 2px solid #e0e0e0;
        }
        .header h1 {
            color: #2c3e50; font-size: 2.5em; margin-bottom: 10px;
        }
        .header .date { color: #7f8c8d; font-size: 1.1em; }
        .stats {
            background: #ecf0f1; padding: 15px; border-radius: 6px;
            margin-bottom: 25px; text-align: center;
        }
        .stats .number { font-size: 2em; font-weight: bold; color: #3498db; }
        .news-list { display: grid; gap: 15px; }
        .news-item {
            background: white; border: 1px solid #e0e0e0;
            border-radius: 8px; padding: 20px; transition: all 0.3s ease;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }
        .news-item:hover {
            transform: translateY(-2px); box-shadow: 0 4px 12px rgba(0,0,0,0.1);
            border-color: #3498db;
        }
        .news-title {
            font-size: 1.2em; font-weight: 600; margin-bottom: 10px; line-height: 1.4;
        }
        .news-title a {
            color: #2c3e50; text-decoration: none; transition: color 0.3s ease;
        }
        .news-title a:hover { color: #3498db; }
        .news-meta {
            display: flex; justify-content: space-between; align-items: center;
            color: #7f8c8d; font-size: 0.9em; margin-top: 10px;
        }
        .source {
            background: #3498db; color: white; padding: 4px 8px;
            border-radius: 4px; font-size: 0.8em;
        }
        .footer {
            text-align: center; margin-top: 40px; padding-top: 20px;
            border-top: 1px solid #e0e0e0; color: #7f8c8d;
        }
        @media (max-width: 768px) {
            .container { margin: 10px; padding: 15px; }
            .header h1 { font-size: 2em; }
            .news-meta {
                flex-direction: column; align-items: flex-start; gap: 5px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“° æ¯æ—¥æ–°é—»æ±‡æ€»</h1>
            <div class="date">{{ date }} Â· è‡ªåŠ¨æ›´æ–°</div>
        </div>
        
        <div class="stats">
            <div>ä»Šæ—¥å…±æ”¶å½• <span class="number">{{ news_count }}</span> æ¡æ–°é—»</div>
        </div>
        
        <div class="news-list">
            {% for news in news_items %}
            <div class="news-item">
                <div class="news-title">
                    <a href="{{ news.link }}" target="_blank" rel="noopener">{{ news.title }}</a>
                </div>
                <div class="news-meta">
                    <span class="source">{{ news.source }}</span>
                    <span class="time">{{ news.timestamp }}</span>
                </div>
            </div>
            {% endfor %}
        </div>
        
        <div class="footer">
            <p>ğŸ¤– ç”± GitHub Action è‡ªåŠ¨æŠ“å–å’Œç”Ÿæˆ</p>
            <p>æ•°æ®æ¥æºï¼š<a href="https://tophub.today/c/news" target="_blank">TopHub ä»Šæ—¥çƒ­æ¦œ</a></p>
        </div>
    </div>
</body>
</html>'''
    
    template = Template(html_template)
    html_content = template.render(
        date=today.strftime('%Yå¹´%mæœˆ%dæ—¥'),
        news_count=len(news_items),
        news_items=news_items
    )
    
    # å†™å…¥æ–‡ä»¶
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(html_content)
    
    print(f"HTMLæ–‡ä»¶å·²ç”Ÿæˆ: {filepath}")
    return filepath

def save_json_data(news_items, base_dir):
    """ä¿å­˜JSONæ ¼å¼çš„æ•°æ®"""
    today = datetime.now()
    filename = f"news-{today.strftime('%Y-%m-%d')}.json"
    filepath = os.path.join(base_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(news_items, f, ensure_ascii=False, indent=2)
    
    print(f"JSONæ•°æ®å·²ä¿å­˜: {filepath}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æŠ“å–TopHubæ–°é—»...")
    
    # åˆ›å»ºç›®å½•ç»“æ„
    base_dir, html_dir = setup_directories()
    
    # æŠ“å–æ–°é—»
    news_items = scrape_tophub_news()
    
    if news_items:
        # ç”ŸæˆHTMLæ–‡ä»¶
        html_file = generate_html(news_items, html_dir)
        
        # ä¿å­˜JSONæ•°æ®
        save_json_data(news_items, base_dir)
        
        print(f"âœ… æˆåŠŸç”Ÿæˆ {len(news_items)} æ¡æ–°é—»è®°å½•")
        print(f"ğŸ“ HTMLæ–‡ä»¶ä½ç½®: {html_file}")
        print(f"ğŸ“ JSONæ•°æ®ä½ç½®: {base_dir}")
    else:
        print("âŒ æ²¡æœ‰è·å–åˆ°æ–°é—»æ•°æ®")

if __name__ == "__main__":
    main()
EOF
        
    - name: Run news scraping
      run: |
        python scrape_news.py
        
    - name: Setup Git configuration
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
    - name: Commit and push changes
      run: |
        if [[ -n $(git status --porcelain) ]]; then
          git add -A
          git commit -m "Auto update news: $(date '+%Y-%m-%d %H:%M:%S')"
          git push
        else
          echo "No changes to commit"
        fi